# config.py
import os
from dotenv import load_dotenv
# Load environment variables from .env file
load_dotenv()

SHOW_CONSOLE_LOGS_IMPORT = True
SHOW_CONSOLE_LOGS_API = True
SHOW_CONSOLE_LOGS_REPORTS = True

# TODO: Make this dynamic from DB models
columns_names_by_table = {
    "departments": ['id', 'department'],
    "jobs": ['id', 'job'],
    "hired_employees": ['id', 'name', 'datetime', 'department_id', 'job_id']
}

# Assuming env variables for both local and cloud environments
instance_connection_name = os.environ.get("INSTANCE_CONNECTION_NAME")
db_host = os.environ.get("DB_HOST")
db_name = os.environ.get("DB_NAME")
db_user = os.environ.get("DB_USER")
db_password = os.environ.get("DB_PASS")
db_port = os.environ.get("DB_PORT")

db_config = {
    "pool_size": 5,
    "max_overflow": 2,
    "pool_timeout": 30,
    "pool_recycle": 1800,
}

# Check if running on Google Cloud SQL o local MySQL 
if os.environ.get("INSTANCE_CONNECTION_NAME"):
    print("CONNECTING TO MYSQL USING SOCKET")
    db_socket_dir = os.environ.get("DB_SOCKET_DIR", "/cloudsql")
    cloud_sql_unix_socket = os.path.join(db_socket_dir, instance_connection_name)
    DATABASE_URI = f"mysql+pymysql://{db_user}:{db_password}@/{db_name}?unix_socket={cloud_sql_unix_socket}"
else:
    print("CONNECTING TO MYSQL USING IP")
    DATABASE_URI = f"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

"""define directories"""
RESULT_FOLDER = 'RESULTS'

UPLOAD_FOLDER = f"{RESULT_FOLDER}/UPLOADS"
LOGS_FOLDER  = f"{RESULT_FOLDER}/LOGS"
BACKUPS_FOLDER = f"{RESULT_FOLDER}/BACKUPS"

# TODO: Consider cloud storge options
 
# create directores if not exist
# main dir for all files generated by the app
if not os.path.exists(RESULT_FOLDER):
    os.makedirs(RESULT_FOLDER)
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)
if not os.path.exists(LOGS_FOLDER):
    os.makedirs(LOGS_FOLDER)
if not os.path.exists(BACKUPS_FOLDER):
    os.makedirs(BACKUPS_FOLDER)
