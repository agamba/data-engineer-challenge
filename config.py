# config.py
import os
from dotenv import load_dotenv

# Set to false if using local MySQL
USE_CLOUD_SQL = False

if USE_CLOUD_SQL and os.environ.get("INSTANCE_CONNECTION_NAME"):
    # Pass env vars to the container (see Dockerfile)
    print("CONNECTING TO MYSQL USING SOCKET IN GCP")
else:
    # Load environment variables from .env file for local development
    load_dotenv()
    print("CONNECTING TO MYSQL USING IP")

# CONTROL LOGS IN TERMINAL
SHOW_CONSOLE_LOGS_IMPORT = True
SHOW_CONSOLE_LOGS_API = True
SHOW_CONSOLE_LOGS_REPORTS = True

instance_connection_name = os.environ.get("INSTANCE_CONNECTION_NAME")
db_host = os.environ.get("DB_HOST")
db_name = os.environ.get("DB_NAME")
db_user = os.environ.get("DB_USER")
db_password = os.environ.get("DB_PASS")
db_port = os.environ.get("DB_PORT")

db_config = {
    "pool_size": 5,
    "max_overflow": 2,
    "pool_timeout": 30,
    "pool_recycle": 1800,
}

# Check if running on Google Cloud SQL o local MySQL 
if USE_CLOUD_SQL and os.environ.get("INSTANCE_CONNECTION_NAME"):
    db_socket_dir = os.environ.get("DB_SOCKET_DIR", "/cloudsql")
    cloud_sql_unix_socket = os.path.join(db_socket_dir, instance_connection_name)
    DATABASE_URI = f"mysql+pymysql://{db_user}:{db_password}@/{db_name}?unix_socket={cloud_sql_unix_socket}"
else:
    DATABASE_URI = f"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

"""define directories"""
RESULT_FOLDER = 'RESULTS'

UPLOAD_FOLDER = f"{RESULT_FOLDER}/UPLOADS"
LOGS_FOLDER  = f"{RESULT_FOLDER}/LOGS"
BACKUPS_FOLDER = f"{RESULT_FOLDER}/BACKUPS"

# TODO: Consider cloud storge options
 
# create directores if not exist
# main dir for all files generated by the app
if not os.path.exists(RESULT_FOLDER):
    os.makedirs(RESULT_FOLDER)
if not os.path.exists(UPLOAD_FOLDER):
    os.makedirs(UPLOAD_FOLDER)
if not os.path.exists(LOGS_FOLDER):
    os.makedirs(LOGS_FOLDER)
if not os.path.exists(BACKUPS_FOLDER):
    os.makedirs(BACKUPS_FOLDER)
